---
title: "Basic Usage of mrgvalprep and mrgvalidate"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{basic_usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

The `mrgvalprep` package contains helper functions for gathering, formatting, and preprocessing input data for `mrgvalidate`. The `mrgvalidate` package is used specifically to generate three documents that are necessary for the software validation process at Mertrum Research Group. Those documents are:

* requirements-specification.docx
* validation-testing.docx
* traceability-matrix.docx

This vignette demonstrates some of the primary functionality for using `mrgvalprep` to pull validate an R package and build validation docs from content stored in either Github issues or Googlesheets.

```{r setup}
library(mrgvalprep)
#library(mrgvalidate)
devtools::load_all("../../mrgvalidate") ##### REPLACE!!!
```

### Initial setup

Before proceeding, you will need to have `ghpm` installed on your system in order to interact with Github. This is an internal Metrum package that handles calls to the Github API. You can try `library(ghpm)` if you're not sure if you have it. If you do not, install it with:

```{r get ghpm, eval = FALSE}
devtools::install_github("metrumresearchgroup/ghpm")
```

Next, you will need to make sure you have a valid token assigned to the relevant environment variable so that `ghpm` can access the Github API. Depending on whether the package you are validating is on Github Enterprise or public Github, you will either need to set `GHE_PAT` or `GITHUB_PAT` respectively. **If you do not have a token** please read an article like [this](https://happygitwithr.com/github-pat.html#github-pat) for instuctions on getting one set up. Then use the following code to set the variable (substituting `GHE_PAT` if appropriate).

```{r set token, eval = FALSE}
Sys.setenv(GITHUB_PAT="your-token") # "your-token" will be the alphanumeric OAuth token you get from Github
```

# `mrgvalidate` inputs

`mrgvalidate` requires two primary inputs:

* A directory of test outputs (`.csv` and `.json` files, described below)
* A tibble of user stories and, optionally, technical requirements

**The format these are expected to conform to is described in `?mrgvalidate::input_formats`.** Below we will discuss how to generate them.

# Test outputs

Currently, `mrgvalprep` only has a formatter for R's `testthat` package, though we are planning to add more functions to take the input from other test frameworks (for example `go test --json` output) that will return a tibble in the same shape as the output from `parse_testthat_list_reporter()`.

While you can call `parse_testthat_list_reporter()` directly, it is likely easier to use the `validate_tests()` wrapper, which does several things:

* Clones the repo of the package being validated from Github at a specified tag
* Installs the package in a temp directory
* Runs the package's test suite with `devtools::test(reporter = testthat::ListReporter)`
* Passes that output to `parse_testthat_list_reporter()` and writes the resulting tibble to a `.csv`
* Calls `get_sys_info()` to capture relevant system and user information, writing that to a `.json`

The results `.csv` and `.json` files can be passed directly to the `auto_test_dir` argument of `mrgvalidate::create_validation_docs()`.

```{r get testing, eval = FALSE}
validate_tests(
  org = "metrumresearchgroup",
  repo = "mrgvalidatetestreference", # fake package used by mrgvalprep test suite
  milestone = "v0.6.0", # the name of the milestone in github
  version = "0.6.0",     # the tag that will be pulled for testing
  out_file = "my_test_output"
)
```

This call will write the `testthat` output to `my_test_output.csv` and the system info (including the commit hash of the git tag you pass to `version`) to `my_test_output.json`.

## Notes on formatting

Both `mrgvalprep` and `mrgvalidate` rely on the input data being structured in a predictable way.

### Release tag
The commit that represents your release must be tagged, ideally as the version number that you are validating.

### Milestone
All issues relevant to the release you are validating _must_ be associated with the same milestone. This may be the same as the tag for the release commit, but it does not necessarily need to be.

### Issue format
The issue parser for creating validation documents requires a very specific format for _all_ issues that are being validated (i.e. associated with the milestone discussed above).

* Summary section
  * There must be a top-level heading entitled `# Summary`
  * This should contain a "user story" describing the functionality
* Tests section
  * There must be a top-level heading entitled `# Tests`
  * This should contain a bulleted list of all the tests that are relevant to this functionality. This list can take two forms:
    * **test Id's -- NEED TO FILL IN AND EXPLAIN THIS** and also how it needs to be in brackets in the test code
    * Test names (i.e. the first arguement to the `test_that()` function).
  * Note that **if an issue has no tests** associated with it, you _still_ need a top-level heading entitled `# Tests` but it should contain only some brief text about why tests are not necessary (with _no_ leading bullets).
* Risk
  * All issues must have a label that conveys the level of risk associated with this functionality.
  * Label must be formatted like `risk: ____`

There should be no other top-level headings. Examples of correctly formatted issues can be seen in the [test reference repo](https://github.com/metrumresearchgroup/mrgvalidatetestreference/issues) used by `mrgvalidate`.


## testthat tests
Similarly, the package you are validating must use the `testthat` package and the tests must be specified in the following way:

* **MAYBE TALK ABOUT TEST ID'S HERE**
* All tests must have a unique name specified as the first arguement to the `test_that()` function (as in, no two tests have the same name).
* As a word of caution, if you are using Test names in your issues then when you change the names of your tests in future versions, you will have to go back and change the names in the issues as well (as described in the "Issue format" section above). This is obviously toilsome and is one of the primary motivations for using Test Id's instead.

**STOPPED HERE!!!!!!!**

## Github issues
The `mrgvalidate` package depends on the issues in Github being in a very specific format. All functionality that is going to be validated must be defined in a Github issue, formatted as described below.




# Generating the docs

The "happy path" to generating the docs is a single function call.

```{r input vars, eval = FALSE}
mrgvalidate::generate_docs(
  org = "metrumresearchgroup",
  repo = "rbabylon",
  milestone = "v0.6.0", # the name of the milestone in github
  version = "0.6.0"     # the tag that will be pulled for testing
)
```

The `generate_docs()` function is a simple wrapper around the functions you'll see below. It uses the defaults to write out three `.docx` files, as well as the `.md` files that are used to render each.

* validation-testing.docx
* requirements-specification.docx
* traceability-matrix.docx

If these three documents look correct after human inspection, you're done!

## Domain argument

There is only one other argument to `generate_docs()`:

* `domain` -- Domain where the repo lives. Either "github.com" or "ghe.metrum.com", defaulting to "github.com"

## Slightly more detail
The specified repo will be cloned at the tag passed to the `version` argument. By default it is cloned to `tempdir()`. The repo is then built and tested with the following:

```
# build, install, and load from current folder
source_path <- devtools::build()
pkg_name <- unlist(strsplit(basename(source_path), "_"))[1]
install.packages(source_path, lib = ".", repos = NULL)
library(pkg_name, lib.loc = ".", character.only = TRUE)
testthat::test_check(pkg_name)
```

The results of these tests are captured and joined against the tests in the Github issues associated with the milestone passed to the `milestone` argument. This data, combined with the user stories in those issues, are used to create the documents.

# Step by step

If the documents produced by `generate_docs()` don't look right, or you need to debug for any reason, you can run the pieces step-by-step. For the demonstration below, we will use the following constants:

```{r vars}
ORG <- "metrumresearchgroup"
REPO <- "rbabylon"
MILESTONE <- "v0.6.0"
TAG <- "0.6.0"
print(glue::glue("{ORG}/{REPO} -- version: {TAG} -- milestone: {MILESTONE}"))
```

## Validation testing

The `write_validation_testing()` function will do the following:

* clone the repo with the specified tag locally (to `tempdir()` by default)
* run `test_check()` in the `"tests"` folder of the cloned repo
* save the tests results to a file (by default `"all_tests.csv"`)
* render the `validation-testing.md` and corresponding `.docx` from those results

```{r val testing, eval = FALSE}
write_validation_testing(
  org = ORG,
  repo = REPO,
  version = TAG
)
```

If you would like to debug each of these steps manually, you can use the following

```{r pull repo}
# pull the repo for a specific tag to a temp directory
commit_hash <- pull_tagged_repo(org = ORG, repo = REPO, tag = TAG)
print(glue::glue("Pulled tag {TAG} -- commit hash: {commit_hash}"))
```

```{r run tests, eval = FALSE}
# run the tests on it
validate_tests(pkg = REPO)
```

The `validate_tests()` function will save the test results to `"all_tests.csv"` by default. Then you can render the markdown from the saved file with the `dry_run = TRUE` argument.

```{r write val fake, eval=FALSE}
# render markdown from previously run tests
write_validation_testing(
  org = ORG,
  repo = REPO,
  version = TAG,
  dry_run = TRUE
)
```

The files output from `dry_run = TRUE` will be marked with a `DRYRUN` identifier at the top, to indicate that the included date stamp does _not_ in fact match when the repo was cloned and the tests were run.

## Release issues

Before rendering either of the other two documents, you will need to create the tibble of issues assigned to this milestone.

```{r release issues, eval=FALSE}
# get issues from Github
release_issues <- get_issues(org = ORG, repo = REPO, mile = MILESTONE)
stories_df <- process_stories(release_issues, org = ORG, repo = REPO)
```

The tibble returned from `get_issues()` will have information relating to each issue in the specified milestone. That is passed to `process_stories()` which joins it to the test results, and `risk` field, and formats them to be rendered into markdown.

## Requirements specification

The `write_requirements()` function takes the tibble returned from `process_stories()` and renders `requirements-specification.md` and corresponding `.docx` file. This document summarizes the requirements detailed in the Github issues.

```{r req, eval=FALSE}
  write_requirements(
    df = stories_df,
    pkg = REPO,
    version = TAG
  )
```

## Traceability matrix

Likewise, the `write_traceability_matrix()` function takes the tibble returned from `process_stories()` and renders `traceability-matrix.md` and corresponding `.docx` file. This document matches the test results against the corresponding Github issues.

```{r trace, eval=FALSE}
  write_traceability_matrix(
    df = stories_df,
    pkg = REPO,
    version = TAG
  )
```


```{r cleanup, include = FALSE}
# I don't actually eval any of these here, but if they are run piece-by-piece this will cleanup the created files
if (fs::file_exists("all_tests.csv")) fs::file_delete("all_tests.csv")
if (fs::file_exists("all_tests.json")) fs::file_delete("all_tests.json")
if (fs::file_exists("requirements-specification.docx")) fs::file_delete("requirements-specification.docx")
if (fs::file_exists("validation-testing.docx")) fs::file_delete("validation-testing.docx")
if (fs::file_exists("traceability-matrix.docx")) fs::file_delete("traceability-matrix.docx")
if (fs::file_exists("requirements-specification.md")) fs::file_delete("requirements-specification.md")
if (fs::file_exists("validation-testing.md")) fs::file_delete("validation-testing.md")
if (fs::file_exists("traceability-matrix.md")) fs::file_delete("traceability-matrix.md")
```
